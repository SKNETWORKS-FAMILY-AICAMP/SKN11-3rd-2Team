import streamlit as st
import os
import logging
import time
from threading import Thread

# torchì™€ transformers ì„í¬íŠ¸ ì „ì— í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ì´í›„ì— torchì™€ transformers ì„í¬íŠ¸
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (í•˜ë“œì½”ë”©)
MODEL_PATH = "Snowfall0601/finetuned-koalpaca-5.8B"  # ê³ ì •ëœ ëª¨ë¸ ê²½ë¡œ
TOKENIZER_PATH = "beomi/KoAlpaca-Polyglot-5.8B"  # í† í¬ë‚˜ì´ì € ê²½ë¡œ

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="KoAlpaca ì±—ë´‡",
    page_icon="ğŸ¦™",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í˜ì´ì§€ ì œëª©
st.title("ğŸ¦™ KoAlpaca íŒŒì¸íŠœë‹ ëª¨ë¸ ì±—ë´‡")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ëª¨ë¸ ì„¤ì •")
    
    # ëª¨ë¸ ì •ë³´ í‘œì‹œ (ìˆ˜ì • ë¶ˆê°€)
    st.info(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {MODEL_PATH}")
    
    temperature = st.slider("ì˜¨ë„ (Temperature)", min_value=0.1, max_value=2.0, value=0.7, step=0.1, 
                           help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
    max_length = st.slider("ìµœëŒ€ ê¸¸ì´", min_value=64, max_value=512, value=256, step=32,
                         help="ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.")
    
    st.markdown("---")
    st.header("ëª¨ë¸ ì •ë³´")
    st.info(f"ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
    st.info(f"í† í¬ë‚˜ì´ì €: {TOKENIZER_PATH}")
    
    st.markdown("---")
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        for key in st.session_state.keys():
            if key.startswith("messages"):
                del st.session_state[key]
        st.session_state.messages = []
        st.experimental_rerun()

# ì»¤ìŠ¤í…€ ì •ì§€ ê¸°ì¤€ í´ë˜ìŠ¤ ì •ì˜
class StopOnTokens(StoppingCriteria):
    def __init__(self, tokenizer, stop_token_ids):
        self.tokenizer = tokenizer
        self.stop_token_ids = stop_token_ids
    
    def __call__(self, input_ids, scores, **kwargs):
        for stop_ids in self.stop_token_ids:
            if input_ids[0][-len(stop_ids):].tolist() == stop_ids:
                return True
        return False

@st.cache_resource(show_spinner=False)
def load_model():
    """
    í•˜ë“œì½”ë”©ëœ ê²½ë¡œì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (Streamlit ìºì‹± ì ìš©)
    """
    with st.spinner("ëª¨ë¸ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."):
        logger.info(f"ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {MODEL_PATH}")
        logger.info(f"í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {TOKENIZER_PATH}")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, legacy=True)
            logger.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
            
            # í† í¬ë‚˜ì´ì €ì— íŒ¨ë”© í† í° ì„¤ì • (ì—†ëŠ” ê²½ìš°)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                logger.info("íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •")
            
            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                torch_dtype=torch.float16,  # ëª¨ë¸ì„ ë°˜ì •ë°€ë„(FP16)ë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
                device_map="auto",  # ìë™ìœ¼ë¡œ GPUì— í• ë‹¹
                low_cpu_mem_usage=True
            )
            
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model, tokenizer, device
        
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

def generate_response(prompt, model, tokenizer, device, max_length=256, temperature=0.7):
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        # KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜
        alpaca_prompt = f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
        logger.info("KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜ ì™„ë£Œ")
        
        # ì…ë ¥ ì¸ì½”ë”© (attention_mask ëª…ì‹œì  í¬í•¨)
        encoded_input = tokenizer(alpaca_prompt, return_tensors="pt", padding=True)
        input_ids = encoded_input["input_ids"].to(device)
        attention_mask = encoded_input["attention_mask"].to(device)
        
        # ì •ì§€ í† í° ì„¤ì •
        stop_words = ["### ì§ˆë¬¸:", "### ë‹µë³€:"]
        stop_token_ids = [tokenizer.encode(word, add_special_tokens=False) for word in stop_words]
        stopping_criteria = StoppingCriteriaList([StopOnTokens(tokenizer, stop_token_ids)])
        
        # ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™”
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # ìƒì„± ë§¤ê°œë³€ìˆ˜ ì„¤ì •
        generation_kwargs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "max_new_tokens": max_length,
            "temperature": temperature,
            "do_sample": True,
            "top_p": 0.95,
            "pad_token_id": tokenizer.pad_token_id,
            "streamer": streamer,
            "stopping_criteria": stopping_criteria
        }
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
        placeholder = st.empty()
        generated_text = ""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        for text in streamer:
            generated_text += text
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì¤‘ê°„ ê²°ê³¼)
            cleaned_text = generated_text
            if "### ë‹µë³€:" in cleaned_text:
                cleaned_text = cleaned_text.split("### ë‹µë³€:")[0]
            if "### ì§ˆë¬¸:" in cleaned_text:
                cleaned_text = cleaned_text.split("### ì§ˆë¬¸:")[0]
            
            # ì—…ë°ì´íŠ¸ëœ í…ìŠ¤íŠ¸ë¥¼ í”Œë ˆì´ìŠ¤í™€ë”ì— í‘œì‹œ
            placeholder.markdown(cleaned_text)
            
            # íƒœê·¸ê°€ ë°œê²¬ë˜ë©´ ìƒì„± ì¤‘ë‹¨
            if "### ë‹µë³€:" in text or "### ì§ˆë¬¸:" in text:
                break
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ì •ë¦¬
        final_text = generated_text
        if "### ë‹µë³€:" in final_text:
            final_text = final_text.split("### ë‹µë³€:")[0]
        if "### ì§ˆë¬¸:" in final_text:
            final_text = final_text.split("### ì§ˆë¬¸:")[0]
        
        # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ìµœì¢… í…ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
        placeholder.markdown(final_text)
        
        return final_text.strip()
    
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

try:
    # ëª¨ë¸ ë¡œë“œ (ìºì‹± ì ìš©) - ì´ì œ ë§¤ê°œë³€ìˆ˜ ì—†ìŒ
    with st.spinner("ëª¨ë¸ ì¤€ë¹„ ì¤‘..."):
        model, tokenizer, device = load_model()
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ëª¨ë¸ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            response = generate_response(
                prompt=prompt, 
                model=model, 
                tokenizer=tokenizer, 
                device=device,
                max_length=max_length,
                temperature=temperature
            )
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": response})
        
except Exception as e:
    st.error(f"ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    import traceback
    st.error(traceback.format_exc())