import streamlit as st
import os
import json
import logging
import time
import torch
from threading import Thread
from typing import List, Dict, Any, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList
from langchain.schema.retriever import BaseRetriever
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.docstore.document import Document
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ëª¨ë¸ ì„¤ì • (í•˜ë“œì½”ë”©)
MODEL_PATH = "Snowfall0601/finetuned-koalpaca-5.8B"  # ê³ ì •ëœ ëª¨ë¸ ê²½ë¡œ
TOKENIZER_PATH = "beomi/KoAlpaca-Polyglot-5.8B"  # í† í¬ë‚˜ì´ì € ê²½ë¡œ
TEMPERATURE = 0.7  # ê³ ì •ëœ ì˜¨ë„ ê°’
MAX_LENGTH = 256  # ê³ ì •ëœ ìµœëŒ€ ê¸¸ì´
RETRIEVAL_K = 3  # ê° ë²¡í„° DBì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

# ë°ì´í„° ê²½ë¡œ ì„¤ì • (í•˜ë“œì½”ë”©)
DATA_DIR = "./data"  # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
VECTOR_DB_DIR = "./vector_db"  # ë²¡í„° DB ì €ì¥ ê²½ë¡œ

# FAISS DB ì €ì¥ ê²½ë¡œ
FAISS_CLASSIFIED_PATH = os.path.join(VECTOR_DB_DIR, "faiss_classified")
FAISS_EXPANDED_PATH = os.path.join(VECTOR_DB_DIR, "faiss_expanded")

# Chroma DB ì €ì¥ ê²½ë¡œ
CHROMA_BABYLOVE_DIR = os.path.join(VECTOR_DB_DIR, "chroma_babylove")
SHOW_REFERENCES = False  # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ ì—¬ë¶€

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë„ˆëŠ” ì´ˆë³´ ë¶€ëª¨ë¥¼ ìœ„í•œ ìœ¡ì•„ ì „ë¬¸ê°€ ì±—ë´‡ì´ì•¼. 
ì•„ë˜ ì œê³µë˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ 
ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì¤˜.
"""

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¦™ ìœ¡ì•„ ì „ë¬¸ê°€ KoAlpaca ì±—ë´‡",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# í˜ì´ì§€ ì œëª©
st.title("ì´ˆë³´ ë¶€ëª¨ë“¤ì˜ ìœ¡ì•„ë¥¼ ë„ì™€ì£¼ëŠ” ë§ˆíŒŒëœ")
st.markdown("---")

# JSON â†’ Document ë³€í™˜ í•¨ìˆ˜
def load_documents_with_metadata(path: str) -> list[Document]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    docs: list[Document] = []

    # classified_contents.json
    if isinstance(data, dict) and all(isinstance(v, dict) and "post" in v for v in data.values()):
        for pid, e in data.items():
            text = e.get("post", "").strip()
            if not text:
                continue
            meta = {"id": pid}
            if "category" in e:    meta["category"]   = e["category"]
            if "confidence" in e:  meta["confidence"] = e["confidence"]
            docs.append(Document(page_content=text, metadata=meta))

    # expanded_info_contents.json
    elif isinstance(data, list) and data and all(isinstance(e, dict) and "comment" in e for e in data):
        for idx, e in enumerate(data):
            text = e.get("comment", "").strip()
            if not text:
                continue
            meta = {"post": e.get("post", ""), "index": idx}
            docs.append(Document(page_content=text, metadata=meta))

    # vector_db_final.json
    elif isinstance(data, list) and data and all(isinstance(e, dict) and "text" in e for e in data):
        for e in data:
            text = e.get("text", "").strip()
            if not text:
                continue
            docs.append(Document(page_content=text, metadata=e.get("metadata", {})))

    else:
        st.error(f"âŒ `{path}`ì˜ JSON êµ¬ì¡°ë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    return docs

# FAISS ë²¡í„° DB ì´ˆê¸°í™”
@st.cache_resource
def init_faiss(path: str, save_path: str):
    """
    FAISS ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•˜ê³  ì €ì¥/ë¡œë“œí•©ë‹ˆë‹¤.
    path: ì›ë³¸ JSON íŒŒì¼ ê²½ë¡œ
    save_path: FAISS DBë¥¼ ì €ì¥í•  ê²½ë¡œ
    """
    try:
        # ì´ë¯¸ ì €ì¥ëœ FAISS DBê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(save_path):
            logger.info(f"ì €ì¥ëœ FAISS DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {save_path}")
            embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
            return FAISS.load_local(save_path, embedding, allow_dangerous_deserialization=True)
        
        # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        logger.info(f"ìƒˆ FAISS DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {path}")
        docs = load_documents_with_metadata(path)
        embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        
        # FAISS DB ìƒì„± ë° ì €ì¥
        db = FAISS.from_documents(docs, embedding=embedding)
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # FAISS DB ì €ì¥
        db.save_local(save_path)
        logger.info(f"FAISS DBë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {save_path}")
        
        return db
    except Exception as e:
        logger.error(f"FAISS ë²¡í„° DB ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return None

# Chroma ë²¡í„° DB ì´ˆê¸°í™”
@st.cache_resource
def init_chroma(path: str, persist_dir: str, collection_name: str):
    """
    Chroma ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•˜ê³  ì €ì¥/ë¡œë“œí•©ë‹ˆë‹¤.
    path: ì›ë³¸ JSON íŒŒì¼ ê²½ë¡œ
    persist_dir: Chroma DBë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    try:
        embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        
        # ì´ë¯¸ ì €ì¥ëœ DBê°€ ìˆìœ¼ë©´ ë¡œë“œë§Œ í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±
        if os.path.isdir(persist_dir) and os.listdir(persist_dir):
            logger.info(f"ì €ì¥ëœ Chroma DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {persist_dir}")
            return Chroma(
                persist_directory=persist_dir,
                embedding_function=embedding,
                collection_name=collection_name
            )
        else:
            logger.info(f"ìƒˆ Chroma DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {path}")
            # ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            os.makedirs(persist_dir, exist_ok=True)
            
            docs = load_documents_with_metadata(path)
            db = Chroma.from_documents(
                documents=docs,
                embedding=embedding,
                persist_directory=persist_dir,
                collection_name=collection_name
            )
            
            # ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ (ì‹¤ì œë¡œëŠ” ìƒì„± ì‹œ ìë™ ì €ì¥ë˜ì§€ë§Œ í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´)
            db.persist()
            logger.info(f"Chroma DBë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {persist_dir}")
            
            return db
    except Exception as e:
        logger.error(f"Chroma ë²¡í„° DB ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return None

# ê²€ìƒ‰ ë„êµ¬ í´ë˜ìŠ¤ ì •ì˜
class SearchTool:
    def __init__(self, name: str, description: str, vector_db: Any, db_type: str):
        self.name = name
        self.description = description
        self.vector_db = vector_db
        self.db_type = db_type
    
    def search(self, query: str) -> str:
        try:
            if self.vector_db is None:
                return f"{self.name} ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ë²¡í„° DBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
            if hasattr(self.vector_db, 'similarity_search'):
                docs = self.vector_db.similarity_search(query, k=RETRIEVAL_K)
                
                results = []
                for i, doc in enumerate(docs):
                    source = f"[{self.db_type} ë¬¸ì„œ {i+1}]"
                    meta = ""
                    if hasattr(doc, 'metadata') and doc.metadata:
                        if 'category' in doc.metadata:
                            meta += f" ì¹´í…Œê³ ë¦¬: {doc.metadata['category']}"
                        if 'post' in doc.metadata and isinstance(doc.metadata['post'], str) and len(doc.metadata['post']) > 0:
                            meta += f" ê´€ë ¨ ê²Œì‹œê¸€: {doc.metadata['post'][:100]}..."
                    
                    results.append(f"{source}{meta}:\n{doc.page_content}")
                
                return "\n\n".join(results) if results else f"{self.name}ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                return f"{self.name}ì€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë²¡í„° DB íƒ€ì…ì…ë‹ˆë‹¤."
        
        except Exception as e:
            logger.error(f"{self.name} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

# ë²¡í„° DBì™€ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
@st.cache_resource
def initialize_search_tools():
    try:
        # ë²¡í„° DB ì´ˆê¸°í™”
        faiss_classified = init_faiss(
            os.path.join(DATA_DIR, "classified_contents.json"),
            FAISS_CLASSIFIED_PATH
        )
        
        faiss_expanded = init_faiss(
            os.path.join(DATA_DIR, "expanded_info_contents.json"),
            FAISS_EXPANDED_PATH
        )
        
        chroma_baby_love = init_chroma(
            os.path.join(DATA_DIR, "vector_db_final.json"), 
            CHROMA_BABYLOVE_DIR, 
            "baby_love_contents"
        )
        
        # ê°œë³„ ê²€ìƒ‰ ë„êµ¬ ìƒì„±
        search_tools = []
        
        if faiss_classified:
            search_tools.append(
                SearchTool(
                    name="ë¶„ë¥˜_ê²Œì‹œê¸€_ê²€ìƒ‰",
                    description="ìœ¡ì•„ ê´€ë ¨ ë¶„ë¥˜ëœ ê²Œì‹œê¸€ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
                    vector_db=faiss_classified,
                    db_type="ë¶„ë¥˜_ê²Œì‹œê¸€"
                )
            )
        
        if faiss_expanded:
            search_tools.append(
                SearchTool(
                    name="í™•ì¥_ì •ë³´_ê²€ìƒ‰",
                    description="ìœ¡ì•„ ê´€ë ¨ ìƒì„¸ ì •ë³´ì™€ ì¶”ê°€ ì„¤ëª…ì´ í¬í•¨ëœ í™•ì¥ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
                    vector_db=faiss_expanded,
                    db_type="í™•ì¥_ì •ë³´"
                )
            )
        
        if chroma_baby_love:
            search_tools.append(
                SearchTool(
                    name="ë² ì´ë¹„ëŸ¬ë¸Œ_ì •ë³´_ê²€ìƒ‰",
                    description="ë² ì´ë¹„ëŸ¬ë¸Œ ì½˜í…ì¸ ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
                    vector_db=chroma_baby_love,
                    db_type="ë² ì´ë¹„ëŸ¬ë¸Œ"
                )
            )
        
        # ë²¡í„° DB ì‚¬ì „ ìƒì„± (ì›ë˜ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´)
        vector_dbs = {
            "faiss_classified": faiss_classified,
            "faiss_expanded": faiss_expanded,
            "chroma_baby_love": chroma_baby_love
        }
        
        return search_tools, vector_dbs, None
    
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return [], {}, None

# ì»¤ìŠ¤í…€ ì •ì§€ ê¸°ì¤€ í´ë˜ìŠ¤ ì •ì˜
class StopOnTokens(StoppingCriteria):
    def __init__(self, tokenizer, stop_token_ids):
        self.tokenizer = tokenizer
        self.stop_token_ids = stop_token_ids
    
    def __call__(self, input_ids, scores, **kwargs):
        for stop_ids in self.stop_token_ids:
            if input_ids[0][-len(stop_ids):].tolist() == stop_ids:
                return True
        return False

def load_model():
    """
    í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (Streamlit ìºì‹± ì ìš©)
    """
    logger.info(f"í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {MODEL_PATH}")
    logger.info(f"í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {TOKENIZER_PATH}")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    logger.info(f"ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}")
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
        logger.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        
        # í† í¬ë‚˜ì´ì €ì— íŒ¨ë”© í† í° ì„¤ì • (ì—†ëŠ” ê²½ìš°)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •")
        
        # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬ì—ì„œ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,  # ëª¨ë¸ì„ ë°˜ì •ë°€ë„(FP16)ë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
            device_map="auto",  # ìë™ìœ¼ë¡œ GPUì— í• ë‹¹
            low_cpu_mem_usage=True
        )
        
        logger.info("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model, tokenizer, device
    
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

# ì í•©í•œ ë„êµ¬ ì„ íƒ ë° ê²€ìƒ‰ ìˆ˜í–‰
def select_and_use_tools(query: str, search_tools: List[SearchTool]) -> str:
    try:
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ ì„ íƒ ë¡œì§
        keywords = {
            "ë¶„ë¥˜_ê²Œì‹œê¸€_ê²€ìƒ‰": ["ë°œë‹¬", "ë‹¨ê³„", "ì¼ë°˜", "ê¸°ë³¸", "ìˆ˜ìœ ", "ì´ìœ ì‹"],
            "í™•ì¥_ì •ë³´_ê²€ìƒ‰": ["ìƒì„¸", "ìì„¸íˆ", "ì‚¬ë¡€", "ì˜ˆì‹œ", "ê²½í—˜"],
            "ë² ì´ë¹„ëŸ¬ë¸Œ_ì •ë³´_ê²€ìƒ‰": ["ì „ë¬¸", "ì¡°ì–¸", "ì˜í•™", "ê±´ê°•", "ì§ˆë³‘", "ì „ë¬¸ê°€"],
        }
        
        query_lower = query.lower()
        selected_tools = []
        
        # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ê¸°ë°˜ ë„êµ¬ ì„ íƒ
        for tool in search_tools:
            if tool.name in keywords:
                for word in keywords[tool.name]:
                    if word in query_lower:
                        selected_tools.append(tool)
                        break
        
        # ì„ íƒëœ ë„êµ¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë„êµ¬ ì‚¬ìš©
        if not selected_tools:
            logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨, ëª¨ë“  ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš©")
            selected_tools = search_tools
        
        # ì„ íƒëœ ë„êµ¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        results = []
        for tool in selected_tools:
            logger.info(f"'{query}'ì— ëŒ€í•´ '{tool.name}' ë„êµ¬ ì‚¬ìš© ì¤‘...")
            result = tool.search(query)
            if not result.endswith("ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."):
                results.append(f"[{tool.name} ê²°ê³¼]\n{result}")
        
        combined_result = "\n\n".join(results)
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(selected_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
        
        return combined_result if results else "ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    except Exception as e:
        logger.error(f"ë„êµ¬ ì„ íƒ ë° ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

def generate_response(prompt, model, tokenizer, device, search_tools=None):
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        retrieved_context = ""
        no_results_found = False
        
        if search_tools:
            # ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ê²€ìƒ‰ ìˆ˜í–‰
            with st.spinner("ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                retrieved_context = select_and_use_tools(prompt, search_tools)
                logger.info("ë„êµ¬ ê¸°ë°˜ ê²€ìƒ‰ ì™„ë£Œ")
                
                # ê²€ìƒ‰ ê²°ê³¼ì— "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ë˜ëŠ” "ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if "ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in retrieved_context or not retrieved_context:
                    no_results_found = True
        
        # ê´€ë ¨ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ë°”ë¡œ ì‘ë‹µ
        if no_results_found:
            final_text = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì£¼ì œë‚˜ ë” ì¼ë°˜ì ì¸ ìœ¡ì•„ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì˜í•´ ì£¼ì‹œë©´ ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            return final_text, retrieved_context
        
        # KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜
        alpaca_prompt = f"### ì‹œìŠ¤í…œ: {SYSTEM_PROMPT}\n\n"
        
        if retrieved_context:
            alpaca_prompt += f"### ë¬¸ë§¥: {retrieved_context}\n\n"
            
        alpaca_prompt += f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
        logger.info("KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜ ì™„ë£Œ")
        
        # ì…ë ¥ ì¸ì½”ë”© (attention_mask ëª…ì‹œì  í¬í•¨)
        encoded_input = tokenizer(alpaca_prompt, return_tensors="pt", padding=True)
        input_ids = encoded_input["input_ids"].to(device)
        attention_mask = encoded_input["attention_mask"].to(device)
        
        # ì •ì§€ í† í° ì„¤ì •
        stop_words = ["### ì§ˆë¬¸:", "### ë‹µë³€:", "### ì‹œìŠ¤í…œ:", "### ë¬¸ë§¥:"]
        stop_token_ids = [tokenizer.encode(word, add_special_tokens=False) for word in stop_words]
        stopping_criteria = StoppingCriteriaList([StopOnTokens(tokenizer, stop_token_ids)])
        
        # ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™”
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # ìƒì„± ë§¤ê°œë³€ìˆ˜ ì„¤ì •
        generation_kwargs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "max_new_tokens": MAX_LENGTH,
            "temperature": TEMPERATURE,
            "do_sample": True,
            "top_p": 0.95,
            "pad_token_id": tokenizer.pad_token_id,
            "streamer": streamer,
            "stopping_criteria": stopping_criteria
        }
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
        placeholder = st.empty()
        generated_text = ""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        for text in streamer:
            generated_text += text
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì¤‘ê°„ ê²°ê³¼)
            cleaned_text = generated_text
            for tag in stop_words:
                if tag in cleaned_text:
                    cleaned_text = cleaned_text.split(tag)[0]
            
            # ì—…ë°ì´íŠ¸ëœ í…ìŠ¤íŠ¸ë¥¼ í”Œë ˆì´ìŠ¤í™€ë”ì— í‘œì‹œ
            placeholder.markdown(cleaned_text)
            
            # íƒœê·¸ê°€ ë°œê²¬ë˜ë©´ ìƒì„± ì¤‘ë‹¨
            if any(tag in text for tag in stop_words):
                break
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ì •ë¦¬
        final_text = generated_text
        for tag in stop_words:
            if tag in final_text:
                final_text = final_text.split(tag)[0]
        
        # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ìµœì¢… í…ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
        placeholder.markdown(final_text)
        
        # ì°¸ì¡° ì •ë³´ ì¶”ì¶œ (í•„ìš”í•œ ê²½ìš°)
        reference_info = ""
        if SHOW_REFERENCES and retrieved_context:
            reference_info = retrieved_context
        
        return final_text.strip(), reference_info
    
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", ""

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.experimental_rerun()

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message.get("references") and SHOW_REFERENCES:
            st.markdown("---")
            st.markdown("**ì°¸ì¡° ì •ë³´:**")
            with st.expander("ì°¸ì¡° ë¬¸ì„œ í™•ì¸"):
                st.markdown(message["references"])

try:
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, device = load_model()
    
    # ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
    search_tools, vector_dbs, _ = initialize_search_tools()
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ìœ¡ì•„ì— ê´€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ëª¨ë¸ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            response, reference_info = generate_response(
                prompt=prompt, 
                model=model, 
                tokenizer=tokenizer, 
                device=device,
                search_tools=search_tools
            )
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ (ì°¸ì¡° ì •ë³´ í¬í•¨)
        st.session_state.messages.append({
            "role": "assistant", 
            "content": response,
            "references": reference_info if SHOW_REFERENCES else ""
        })
        
except Exception as e:
    st.error(f"ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.error("ëª¨ë¸ ì„¤ì •ê³¼ ë°ì´í„° íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
