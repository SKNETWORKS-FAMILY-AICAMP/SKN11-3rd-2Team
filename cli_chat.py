import torch
import os
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList
from threading import Thread

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê²½ë¡œ ì„¤ì •
MODEL_PATH = "Snowfall0601/finetuned-koalpaca-5.8B"  # í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬
TOKENIZER_PATH = "beomi/KoAlpaca-Polyglot-5.8B"  # í† í¬ë‚˜ì´ì € ê²½ë¡œ

# ì»¤ìŠ¤í…€ ì •ì§€ ê¸°ì¤€ í´ë˜ìŠ¤ ì •ì˜
class StopOnTokens(StoppingCriteria):
    def __init__(self, tokenizer, stop_token_ids):
        self.tokenizer = tokenizer
        self.stop_token_ids = stop_token_ids
    
    def __call__(self, input_ids, scores, **kwargs):
        for stop_ids in self.stop_token_ids:
            if input_ids[0][-len(stop_ids):].tolist() == stop_ids:
                return True
        return False

def load_model(model_path, tokenizer_path):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    - ëª¨ë¸ì€ í—ˆê¹…í˜ì´ìŠ¤ ë ˆí¬ì§€í† ë¦¬ì—ì„œ ë¡œë“œ
    - í† í¬ë‚˜ì´ì €ëŠ” ì§€ì •ëœ ê²½ë¡œì—ì„œ ë¡œë“œ
    """
    logger.info(f"ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_path}")
    logger.info(f"í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {tokenizer_path}")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}")
    
    try:
        # ë‹¤ì–‘í•œ ì˜µì…˜ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„
        try:
            logger.info("ì²« ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„")
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path,
                use_fast=False,
                trust_remote_code=True
            )
        except Exception as e:
            logger.warning(f"ì²« ë²ˆì§¸ ë°©ë²• ì‹¤íŒ¨: {str(e)}")
            logger.info("ë‘ ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„")
            try:
                from transformers import PreTrainedTokenizerFast
                tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
            except Exception as e2:
                logger.warning(f"ë‘ ë²ˆì§¸ ë°©ë²• ì‹¤íŒ¨: {str(e2)}")
                logger.info("ë§ˆì§€ë§‰ ë°©ë²•ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„")
                tokenizer = AutoTokenizer.from_pretrained(
                    tokenizer_path, 
                    padding_side='left',
                    truncation_side='left'
                )
        
        logger.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        
        # í† í¬ë‚˜ì´ì €ì— í•„ìš”í•œ í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •")
        
        # ëª¨ë¸ ì„¤ì •
        model_loading_args = {
            "torch_dtype": torch.float16,
            "device_map": "cuda:0" if device == "cuda" else "auto",
            "low_cpu_mem_usage": True,
            "trust_remote_code": True
        }
        
        # í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œë„
        try:
            logger.info("ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„")
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_loading_args)
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.info("ì¶”ê°€ ì˜µì…˜ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¶”ê°€ ì˜µì…˜ ì‹œë„
            model_loading_args["revision"] = "main"  # ë©”ì¸ ë¸Œëœì¹˜ ì‚¬ìš©
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_loading_args)
        
        logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model, tokenizer, device
    
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

def generate_response(prompt, model, tokenizer, device, max_length=256, temperature=0.7):
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜
        alpaca_prompt = f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
        logger.info("KoAlpaca í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í™˜ ì™„ë£Œ")
        
        try:
            # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í† í°í™”í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
            tokens = tokenizer.tokenize(alpaca_prompt)
            token_ids = tokenizer.convert_tokens_to_ids(tokens)
            input_ids = torch.tensor([token_ids]).to(device)
            attention_mask = torch.ones_like(input_ids).to(device)
            
            logger.info("ìˆ˜ë™ í† í°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"í† í°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ë°±ì—… ë°©ë²•: ì§ì ‘ ì¸ì½”ë”©
            try:
                input_ids = tokenizer.encode(alpaca_prompt, return_tensors="pt").to(device)
                attention_mask = torch.ones_like(input_ids).to(device)
                logger.info("ë°±ì—… ì¸ì½”ë”© ë°©ë²• ì‚¬ìš©")
            except Exception as e2:
                logger.error(f"ë°±ì—… ì¸ì½”ë”© ì¤‘ ì˜¤ë¥˜: {str(e2)}")
                raise
        
        # ì •ì§€ í† í° ì„¤ì •
        stop_words = ["### ì§ˆë¬¸:", "### ë‹µë³€:"]
        stop_token_ids = []
        for word in stop_words:
            try:
                ids = tokenizer.encode(word, add_special_tokens=False)
                stop_token_ids.append(ids)
            except Exception as e:
                logger.error(f"ì •ì§€ í† í° ì¸ì½”ë”© ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")
        
        stopping_criteria = StoppingCriteriaList([StopOnTokens(tokenizer, stop_token_ids)])
        
        # ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™” (ë‹µë³€ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •)
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # ìƒì„± ë§¤ê°œë³€ìˆ˜ ì„¤ì •
        generation_kwargs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "max_new_tokens": max_length,
            "temperature": temperature,
            "do_sample": True,
            "top_p": 0.95,
            "pad_token_id": tokenizer.pad_token_id,
            "streamer": streamer
        }
        
        # stopping_criteriaê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€
        if stop_token_ids:
            generation_kwargs["stopping_criteria"] = stopping_criteria
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
        generated_text = ""
        print("\nëª¨ë¸ ì‘ë‹µ: ", end="", flush=True)
        for text in streamer:
            print(text, end="", flush=True)
            generated_text += text
            
            # ì‘ë‹µì— "### ë‹µë³€:" ë˜ëŠ” "### ì§ˆë¬¸:"ì´ í¬í•¨ë˜ë©´ í•´ë‹¹ ë¶€ë¶„ê¹Œì§€ë§Œ ì‚¬ìš©
            if "### ë‹µë³€:" in text or "### ì§ˆë¬¸:" in text:
                break
                
        print("\n")
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
        # "### ë‹µë³€:" ì´í›„ì˜ ì¶”ê°€ íƒœê·¸ ì œê±°
        cleaned_text = generated_text
        if "### ë‹µë³€:" in cleaned_text:
            cleaned_text = cleaned_text.split("### ë‹µë³€:")[0]
        if "### ì§ˆë¬¸:" in cleaned_text:
            cleaned_text = cleaned_text.split("### ì§ˆë¬¸:")[0]
            
        return cleaned_text.strip()
        
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def chat_with_model():
    """
    ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    try:
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer, device = load_model(MODEL_PATH, TOKENIZER_PATH)
        
        print(f"ğŸ¤– ëª¨ë¸ '{MODEL_PATH}'ì´(ê°€) ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ”¤ í† í¬ë‚˜ì´ì €ëŠ” '{TOKENIZER_PATH}'ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        conversation_history = ""  # ëŒ€í™” ê¸°ë¡ ì €ì¥ìš©
        
        while True:
            user_input = input("\nì‚¬ìš©ì: ")
            
            # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
            if user_input.lower() in ["exit", "quit", "q"]:
                print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            
            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            if conversation_history:
                conversation_history += f"\nì‚¬ìš©ì: {user_input}"
            else:
                conversation_history = f"ì‚¬ìš©ì: {user_input}"
            
            # ì‚¬ìš©ì ì…ë ¥ ê·¸ëŒ€ë¡œ ì „ë‹¬ (KoAlpaca í˜•ì‹ì€ generate_response ë‚´ì—ì„œ ë³€í™˜)
            response = generate_response(user_input, model, tokenizer, device, 500, 0.7)
            
            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            conversation_history += f"\nëª¨ë¸: {response}"
            
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    chat_with_model()