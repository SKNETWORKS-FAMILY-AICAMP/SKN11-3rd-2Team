{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42db1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "try:\n",
    "    with open(\"./vector_db_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"íŒŒì¼ ì—†ìŒ\")\n",
    "    exit()\n",
    "\n",
    "# 2. Document êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=item[\"text\"],\n",
    "        metadata=item[\"metadata\"]\n",
    "    )\n",
    "    for item in raw_data\n",
    "]\n",
    "\n",
    "# 3. OpenAI ì„ë² ë”© ì´ˆê¸°í™”\n",
    "embed_model = OpenAIEmbeddings(\n",
    "\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=key\n",
    "    )  # ë˜ëŠ” \"text-embedding-3-large\"\n",
    "\n",
    "# 4. ChromaDBì— ì €ì¥\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embed_model,\n",
    "    persist_directory=\"./chroma_childcare\"\n",
    ")\n",
    "\n",
    "# 5. ì €ì¥\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a6323db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI  # ë˜ëŠ” OpenAI()\n",
    "\n",
    "# # ğŸ’¬ ì‚¬ìš©ì ì§ˆì˜ì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=\"\"\"\n",
    "# ë‹¹ì‹ ì€ ì˜ìœ ì•„ ë¶€ëª¨ë¥¼ ìœ„í•œ ìœ¡ì•„ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n",
    "# ì•„ë˜ ë¬¸ë§¥(Context)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.\n",
    "# ë§Œì•½ \n",
    "\n",
    "# [ë¬¸ë§¥]\n",
    "# {context}\n",
    "\n",
    "# [ì§ˆë¬¸]\n",
    "# {question}\n",
    "\n",
    "# [ë‹µë³€]\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# # ğŸ” ë²¡í„° DBë¥¼ ê¸°ë°˜ìœ¼ë¡œ retriever ìƒì„±\n",
    "# retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "# # ğŸ’¡ ChatGPT ë˜ëŠ” GPT-4o ëª¨ë¸ í™œìš©\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5, openai_api_key=key),\n",
    "#     retriever=retriever,\n",
    "#     chain_type=\"stuff\",  # stuffëŠ” ë‹¨ìˆœí•œ ë¬¸ì„œ ì‚½ì… ë°©ì‹\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "\n",
    "# query = \" ì•„ì´ê°€ ë°¤ì— ìê¾¸ ê¹¨ëŠ”ë° ì–´ë–»ê²Œ ì¬ì›Œì•¼ í•˜ë‚˜ìš”?\"\n",
    "# # docs = vectordb.similarity_search(query, k=5)\n",
    "# response = qa_chain.run(query)\n",
    "\n",
    "# print(\"ğŸ“Œ ì§ˆë¬¸:\", query)\n",
    "# print(\"ğŸ§  ë‹µë³€:\", response)\n",
    "\n",
    "# # for doc in docs:\n",
    "# #     print(f\"[{doc.metadata['category_name']} / {doc.metadata['page_name']}]\")\n",
    "# #     print(doc.page_content)\n",
    "# #     print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2af6e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ì´ê°€ ì—´ì´ ë§ì´ ë‚˜ìš”\n",
      "ğŸ¤– ì•„ê¸°ì˜ ê°œì›” ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš” ğŸ˜Š\n",
      "6ê°œì›”ì´ìš”\n",
      "ğŸ§  ë‹µë³€: ì•„ì´ê°€ ì—´ì´ ë‚˜ëŠ” ìƒí™©ì€ ë¶€ëª¨ì—ê²Œ ê±±ì •ìŠ¤ëŸ¬ìš´ ì¼ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 6ê°œì›” ì•„ê¸°ì˜ ê²½ìš°, ì—´ì´ ë‚˜ëŠ” ì›ì¸ì€ ë‹¤ì–‘í•  ìˆ˜ ìˆìœ¼ë©°, ê°ê¸°, ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼, ì˜ˆë°© ì ‘ì¢… í›„ ë°˜ì‘ ë“±ì´ ì¼ë°˜ì ì¸ ì›ì¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì•„ì´ê°€ ì—´ì´ ë‚  ë•Œ ì·¨í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì¡°ì¹˜ì…ë‹ˆë‹¤:\n",
      "\n",
      "1. **ì²´ì˜¨ ì¸¡ì •**: ì•„ê¸°ì˜ ì²´ì˜¨ì„ ì •í™•í•˜ê²Œ ì¸¡ì •í•˜ì—¬ ì—´ì´ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. 38ë„ ì´ìƒì´ë©´ ì—´ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ìˆ˜ë¶„ ê³µê¸‰**: ì—´ì´ ë‚  ë•ŒëŠ” íƒˆìˆ˜ ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ì•„ê¸°ì—ê²Œ ì¶©ë¶„í•œ ìˆ˜ë¶„ì„ ê³µê¸‰í•˜ì„¸ìš”. ëª¨ìœ  ìˆ˜ìœ  ì¤‘ì´ë¼ë©´ ìì£¼ ìˆ˜ìœ í•˜ê³ , ë¶„ìœ ë¥¼ ë¨¹ëŠ” ì•„ê¸°ë¼ë©´ ë¬¼ì„ ì¡°ê¸ˆì”© ìì£¼ ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ì˜·ì°¨ë¦¼ ì¡°ì ˆ**: ì•„ê¸°ê°€ ë„ˆë¬´ ë¥ì§€ ì•Šë„ë¡ ê°€ë²¼ìš´ ì˜·ì„ ì…íˆê³ , ë°© ì•ˆì˜ ì˜¨ë„ë¥¼ ì ì ˆí•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.\n",
      "\n",
      "4. **í•´ì—´ì œ ì‚¬ìš©**: í•„ìš”í•˜ë‹¤ë©´ ì†Œì•„ê³¼ ì˜ì‚¬ì™€ ìƒë‹´ í›„ ì•„ê¸°ì—ê²Œ ì ì ˆí•œ í•´ì—´ì œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íœ(íƒ€ì´ë ˆë†€)ì´ë‚˜ ì´ë¶€í”„ë¡œíœ(ì–´ë¦°ì´ìš© ë¶€ë£¨íœ)ì´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì˜ì‚¬ì˜ ì§€ì‹œ ì—†ì´ ì•½ì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.\n",
      "\n",
      "5. **ì˜ì‚¬ ìƒë‹´**: ì—´ì´ 24ì‹œê°„ ì´ìƒ ì§€ì†ë˜ê±°ë‚˜, ì•„ê¸°ê°€ ë§¤ìš° ë³´ì±„ê±°ë‚˜, ì‹ìš•ì´ ì—†ê³ , ê¸°ìš´ì´ ì—†ì–´ ë³´ì´ëŠ” ê²½ìš° ì¦‰ì‹œ ì†Œì•„ê³¼ ì˜ì‚¬ì™€ ìƒë‹´í•˜ì„¸ìš”.\n",
      "\n",
      "ì•„ê¸°ì˜ ìƒíƒœë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ê³ , í•„ìš”ì‹œì—ëŠ” ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì•„ê¸°ì˜ ë¹ ë¥¸ íšŒë³µì„ ë°”ëë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import re\n",
    "\n",
    "def detect_missing_info(question: str) -> str | None:\n",
    "    if not re.search(r'\\b(\\d{1,2}\\s*ê°œì›”|[0-9]{1,2}ì‚´|ì‹ ìƒì•„|ì¶œìƒ|ëŒ|ë§Œ\\s*\\d{1,2})\\b', question):\n",
    "        return \"ì•„ê¸°ì˜ ê°œì›” ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš” ğŸ˜Š\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ë©”ëª¨ë¦¬\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "    template=\"\"\"\n",
    "ë‹¹ì‹ ì€ ì˜ìœ ì•„ ë¶€ëª¨ì˜ ìœ¡ì•„ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µí•˜ëŠ” ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n",
    "ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history}\n",
    "\n",
    "[ë¬¸ë§¥ ì •ë³´]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "[ë‹µë³€]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Vector DB ê¸°ë°˜ retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Conversational Retrieval QA ì²´ì¸\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o\", temperature=0.3, openai_api_key=key),\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸\n",
    "user_question = input(\"â“ì•ˆë…•í•˜ì„¸ìš”. ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”\")\n",
    "print(user_question)\n",
    "\n",
    "# 1. ê°œì›” ìˆ˜ ë¹ ì¡ŒëŠ”ì§€ í™•ì¸\n",
    "followup_msg = detect_missing_info(user_question)\n",
    "if followup_msg:\n",
    "    print(\"ğŸ¤–\", followup_msg)\n",
    "    # ì‚¬ìš©ì ì‘ë‹µ ì˜ˆì‹œ\n",
    "    user_reply = input()\n",
    "    print(user_reply)\n",
    "    final_input = user_question + \" (ì•„ì´ ê°œì›” ìˆ˜: \" + user_reply + \")\"\n",
    "else:\n",
    "    final_input = user_question\n",
    "\n",
    "# 2. ìµœì¢… ì§ˆì˜ ì²˜ë¦¬\n",
    "response = qa.run(final_input)\n",
    "print(\"ğŸ§  ë‹µë³€:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
